{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIvukbBRCpuz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5946d7a-a4b7-40cc-9b59-b51c108ed807"
      },
      "source": [
        "pip install pyyaml h5py "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5; python_version == \"3.7\" in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqFe9trRXEFL",
        "outputId": "93b146b4-4410-4625-e6cb-50d3b9367550"
      },
      "source": [
        "pip install python-twitter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-twitter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/2eb36853d8ca49a70482e2332aa5082e09b3180391671101b1612e3aeaf1/python_twitter-3.5-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 21.2MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 16.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 40kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from python-twitter) (0.16.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from python-twitter) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from python-twitter) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->python-twitter) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->python-twitter) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->python-twitter) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->python-twitter) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->python-twitter) (3.0.4)\n",
            "Installing collected packages: python-twitter\n",
            "Successfully installed python-twitter-3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as1Zn00uCPbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7e70a3-3cd2-443e-c2b9-715978a6b973"
      },
      "source": [
        "import twitter\n",
        "import os\n",
        "import numpy as np\n",
        "# from config import *\n",
        "import time\n",
        "import sys\n",
        "import pickle\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,LSTM,Dropout,BatchNormalization,Dense,Conv1D,Bidirectional,MaxPooling1D,Flatten, GlobalMaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard\n",
        "# from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import TensorBoard\n",
        "print('Is using GPU?', tf.test.is_gpu_available())\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "NAME = f\"Model-W-LSTM-PRED-{int(time.time())}\"\n",
        "\n",
        "\n",
        "# initialize api instance\n",
        "api = twitter.Api(consumer_key='gLfZFnqWAcFqJQ2VtlbKtG1n7',\n",
        "                 consumer_secret='ND0Ld0ZzjA8I9gn0fBNNsgAQNrdYTgM7sfqPosYoODaEoPIPtL',\n",
        "                 access_token_key='2331621302-fs8VOMckbASRCimYQyxbDXdoLPckea4k5SYusMD',\n",
        "                 access_token_secret='Ww09Z4FnZplxAzdp53nKOJB2xDg32WQjD2VSFTYc8dHIb')\n",
        "\n",
        "#print(consumer_key,consumer_secret,access_token_key,access_token_secret)\n",
        "\n",
        "# test authentication\n",
        "print(api.VerifyCredentials())\n",
        "# print(twitter_api.GetStatus(126402758403305000))\n",
        "\n",
        "def create_Test(search):\n",
        "  try:\n",
        "    fetched=api.GetSearch(search,count=98,lang=\"en\",result_type=\"popular\")\n",
        "    print(len(fetched))\n",
        "    return [{\"id\":status.id,\"text\":status.text,\"label\":None} for status in fetched]\n",
        "  except:\n",
        "    return None\n",
        "def preprocess_text(self,text):\n",
        "    a\n",
        "    stop_words = stopwords.words('english')\n",
        "    text_remove_symbol='\\$[A-Za-z]*'\n",
        "    text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "    text_hash=r'#([^\\s]+)'\n",
        "    text=re.sub(text_remove_symbol,'',str(text).lower()).strip()\n",
        "    text=re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
        "    text=re.sub(text_hash,' ',str(text)).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if  token not in stop_words:\n",
        "            tokens.append(lemmatizer.lemmatize(token,get_wordnet_pos(token)))\n",
        "    return \" \".join(tokens)\n",
        "def get_wordnet_pos(self,word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict={\n",
        "        'J':wordnet.ADJ,\n",
        "        'N':wordnet.NOUN,\n",
        "        'V':wordnet.VERB,\n",
        "        'R':wordnet.ADV\n",
        "        }\n",
        "    return tag_dict.get(tag,wordnet.NOUN)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-10127a9415d2>:26: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "Is using GPU? False\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "{\"created_at\": \"Fri Feb 07 09:58:39 +0000 2014\", \"default_profile\": true, \"favourites_count\": 1, \"followers_count\": 11, \"friends_count\": 37, \"id\": 2331621302, \"id_str\": \"2331621302\", \"name\": \"Abdul Dalvi\", \"profile_background_color\": \"C0DEED\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/431732476038221824/gUvU1U2Z_normal.jpeg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/431732476038221824/gUvU1U2Z_normal.jpeg\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"screen_name\": \"AbdulDalvi\", \"status\": {\"created_at\": \"Sat Sep 17 03:08:20 +0000 2016\", \"favorite_count\": 1, \"id\": 776981084818935808, \"id_str\": \"776981084818935808\", \"lang\": \"en\", \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\", \"text\": \"Join the movement. Take action and become a #globalcitizen. https://t.co/dk7k5gvBFG\"}, \"statuses_count\": 3, \"withheld_in_countries\": []}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_j2xCGdbJS1"
      },
      "source": [
        "# import twitter\n",
        "import os\n",
        "import numpy as np\n",
        "#from config import *\n",
        "import time\n",
        "import sys\n",
        "import pickle\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,LSTM,Dropout,BatchNormalization,Dense,Conv1D,Bidirectional,MaxPooling1D,Flatten, GlobalMaxPooling1D, Activation\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrLoqzi57Whz"
      },
      "source": [
        "embedding_matrix=np.zeros((vocab_size,100))\n",
        "for word,i in token.word_index.items():\n",
        "  embedding_value=embeddings_dict.get(word)\n",
        "  if embedding_value is not None:\n",
        "    embedding_matrix[i]=embedding_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCYwuUSq7XOP"
      },
      "source": [
        "embedding_matrix=np.zeros((vocab_size,100))\n",
        "for word,i in token.word_index.items():\n",
        "  embedding_value=embeddings_dict.get(word)\n",
        "  if embedding_value is not None:\n",
        "    embedding_matrix[i]=embedding_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd_okz3f7Xy-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMJU-U8dClci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba8eec72-4fe6-41aa-b0af-042aee550f46"
      },
      "source": [
        "#-------------------------RUN TO IMPORT EMBEDDING DICTIONARY------------------------------\n",
        "pickle_in=open(\"/content/drive/My Drive/Colab Notebooks/embeddings_dict.pickle\",\"rb\")\n",
        "embeddings_dict=pickle.load(pickle_in)\n",
        "print(type(embeddings_dict))\n",
        "print(len(embeddings_dict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "1193514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUk4VUHuClZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2073a2db-4d7b-4a37-fce2-14609b0a208c"
      },
      "source": [
        "!pip3 install pickle5\n",
        "import pickle5 as pickle\n",
        "#-------------------------RUN TO IMPORT PROCESSED TWEETS------------------------------\n",
        "pickle_in=open(\"/content/drive/MyDrive/Colab Notebooks/BE_PROJECT/Data/processed_stockTwits.pkl\",\"rb\")\n",
        "df=pickle.load(pickle_in)\n",
        "print(type(df))\n",
        "print(len(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "106218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfW0jwrZWeCX"
      },
      "source": [
        "def p1(x):\n",
        "    if x==-1:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GrDT7y92WYcv",
        "outputId": "8339ede0-f62d-4ce6-868e-b4fd51d50f89"
      },
      "source": [
        "df['sentiment']=df['sentiment'].apply(lambda x:p1(x))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>target tech turnover darling every resistance ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>could buyer region move holiday week</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yesterday technically count green long term ca...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>like area call opportunity expiry month would ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>analyst alert call able close quick scalp gain...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message  sentiment\n",
              "0  target tech turnover darling every resistance ...          1\n",
              "1               could buyer region move holiday week          0\n",
              "2  yesterday technically count green long term ca...          1\n",
              "3  like area call opportunity expiry month would ...          1\n",
              "4  analyst alert call able close quick scalp gain...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKvUaipKNabH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d3a437-0d0a-412d-9b5a-4c520ad894f8"
      },
      "source": [
        "df_train,df_test=train_test_split(df,test_size=0.2,random_state=42)\n",
        "print(df_train.head())\n",
        "\n",
        "x_train=df_train.message\n",
        "y_train=df_train.sentiment\n",
        "x_test=df_test.message\n",
        "y_test=df_test.sentiment\n",
        "\n",
        "print(x_train.shape,y_train.shape)\n",
        "\n",
        "token=Tokenizer()\n",
        "token.fit_on_texts(x_train)\n",
        "seq=token.texts_to_sequences(x_train)\n",
        "x_train=pad_sequences(seq,maxlen=30)\n",
        "seq=token.texts_to_sequences(x_test)\n",
        "x_test=pad_sequences(seq,maxlen=30)\n",
        "print(x_train.shape,x_test.shape)\n",
        "\n",
        "vocab_size=len(token.word_index.items())+1\n",
        "print(vocab_size)\n",
        "\n",
        "labels=y_train.unique().tolist()\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                 message  sentiment\n",
            "50265  look weak sell open never neutral mute trader ...          0\n",
            "94671  theta kill call triangle wave position usually...          1\n",
            "62152  give next bitcoin news story chinese court rul...          1\n",
            "2171   wait asia wake hears jpow news party gonna go ...          1\n",
            "5651     let gooooo bull consolidation next rippinnnnnnn          1\n",
            "(84974,) (84974,)\n",
            "(84974, 30) (21244, 30)\n",
            "28546\n",
            "[0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "q7mM1crUDKl6",
        "outputId": "176a8993-a34d-49e8-87fb-325cef812258"
      },
      "source": [
        "len(token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-fd193e8c368e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'Tokenizer' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8cDhYR0Avuq",
        "outputId": "500b99ad-3971-4b6d-ea1f-e8be5be4fdf5"
      },
      "source": [
        "tokenizer=token.to_json()\n",
        "print(type(tokenizer))\n",
        "import pickle\n",
        "pickling_on = open('/content/drive/MyDrive/Colab Notebooks/BE_PROJECT/tokenizer.pkl',\"wb\")\n",
        "pickle.dump(tokenizer, pickling_on)\n",
        "pickling_on.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "6FDX3ksNyrAz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtreJQfPDnSr",
        "outputId": "b90204d4-8850-4423-a795-bf5a7c9ee3ff"
      },
      "source": [
        "tokenizer=tf.keras.preprocessing.text.tokenizer_from_json(tokenizer)\n",
        "print(type(tokenizer))\n",
        "vocab_size=len(tokenizer.word_index.items())+1\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'keras_preprocessing.text.Tokenizer'>\n",
            "28546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVlpUVyB5DJW",
        "outputId": "88f4c51e-14fb-4dda-b4b6-7554b6716172"
      },
      "source": [
        "# RUN TO IMPORT TOKENIZER\n",
        "pickle_in=open(\"/content/drive/MyDrive/Colab Notebooks/BE_PROJECT/tokenizer.pkl\",\"rb\")\n",
        "tokenizer=pickle.load(pickle_in)\n",
        "print(type(tokenizer))\n",
        "token=tf.keras.preprocessing.text.tokenizer_from_json(tokenizer)\n",
        "vocab_size=len(token.word_index.items())+1\n",
        "print(vocab_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnUIe7j6W6Rb",
        "outputId": "892ca260-2a46-4985-94ff-c2a2fe4322ee"
      },
      "source": [
        "y_train=np.array(y_train).reshape(-1,1)\n",
        "y_test=np.array(y_test).reshape(-1,1)\n",
        "print(y_train.shape,y_test.shape)\n",
        "print(x_train.shape,x_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(84974, 1) (21244, 1)\n",
            "(84974, 30) (21244, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y72ZaZMNXc1E",
        "outputId": "5e4e8d33-a0a3-407f-e656-de3419dbfeda"
      },
      "source": [
        "pickle_in=open(\"/content/drive/MyDrive/Colab Notebooks/BE_PROJECT/Data/embd.pkl\",\"rb\")\n",
        "embd=pickle.load(pickle_in)\n",
        "print(type(embd))\n",
        "print(len(embd))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "31709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM & LR**"
      ],
      "metadata": {
        "id": "GIf2tH5xyfrh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOxdWDGHXcyQ"
      },
      "source": [
        "#Importing SVM & LR\n",
        "from sklearn import svm\n",
        " from sklearn.linear_model import LinearRegression\n",
        "clf=svm.SVC(kernel=\"sigmoid\")\n",
        "clf.fit(x_train,y_train)\n",
        "clf.score\n",
        "reg = LinearRegression().fit(X, y)\n",
        "reg.score(x_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyPAlwNeXcwc"
      },
      "source": [
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fonjsUBwXctc"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PafKOaNwXcmj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network begin**"
      ],
      "metadata": {
        "id": "xwJFqLj3ykun"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1WaS6cXNaX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ac7a0d-da95-463c-cb4e-3e4a9ab00cf0"
      },
      "source": [
        "encoder=preprocessing.LabelEncoder()\n",
        "encoder.fit(y_train.tolist())\n",
        "y_train=encoder.transform(y_train.tolist())\n",
        "y_test=encoder.transform(y_test.tolist())\n",
        "\n",
        "y_train=y_train.reshape(-1,1)\n",
        "y_test=y_test.reshape(-1,1)\n",
        "\n",
        "print(y_train.shape,y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1279999, 1) (320000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARgcAK2oUACC"
      },
      "source": [
        "embedding_matrix=np.zeros((vocab_size,100))\n",
        "for word,i in token.word_index.items():\n",
        "  embedding_value=embeddings_dict.get(word)\n",
        "  if embedding_value is not None:\n",
        "    embedding_matrix[i]=embedding_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPX9BSbQ6gwL"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op7wAGrsNaWI"
      },
      "source": [
        "embedding_layer=Embedding(input_dim=vocab_size,output_dim=100,input_length=30,weights=[embedding_matrix],trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRfp-msXZX1g",
        "outputId": "19cb7f65-e94f-4097-b69e-cac2ba16cab3"
      },
      "source": [
        "print(x_train.shape,x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(84974, 30) (21244, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZD46_E6NaRv"
      },
      "source": [
        "model=Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv1D(128,5,activation=\"relu\"))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv1D(256,5,activation=\"relu\"))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=256,activation=\"relu\"))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# model.add(Bidirectional(LSTM(128,activation=\"relu\",recurrent_dropout=0.2)))\n",
        "model.add(Dense(units=1,activation=\"sigmoid\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ucxgRhBNaPr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pq82SRnu_Ox"
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2,activation=\"relu\")))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(250,activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(250,activation=\"relu\"))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIWvK-3nZsyV",
        "outputId": "5883708b-61dd-434e-c61b-fa0119a206a6"
      },
      "source": [
        "NUM_FILTERS = 250\n",
        "KERNEL_SIZE = 3\n",
        "HIDDEN_DIMS = 250\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into EMBEDDING_DIM dimensions\n",
        "model.add(embedding_layer)\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# we add a Convolution1D, which will learn NUM_FILTERS filters\n",
        "model.add(Conv1D(NUM_FILTERS,\n",
        "                 KERNEL_SIZE,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "\n",
        "# we use max pooling:\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "model.add(Dense(HIDDEN_DIMS))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHtK-fNmE2VF"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into EMBEDDING_DIM dimensions\n",
        "model.add(embedding_layer)\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(64, recurrent_dropout=0.2,activation=\"relu\"))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJXAcz9Ru_M-"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO0CANqtu_LZ"
      },
      "source": [
        "print(np.unique(y_train,return_counts=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj-uYF0eOuPW"
      },
      "source": [
        "x_train[0],y_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jjR3ZN3u_JX"
      },
      "source": [
        "h=model.fit(x_train,y_train,epochs=3,batch_size=254,validation_split=0.2,verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "953AxQkxapAl"
      },
      "source": [
        "score, acc = model.evaluate(x_test, y_test, batch_size=256)\n",
        "print('\\nAccuracy: ', acc*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdPwtC6sJsST"
      },
      "source": [
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKN-QFFv3FSd"
      },
      "source": [
        "model.save('/content/drive/MyDrive/Colab Notebooks/BE_PROJECT/SM_CNNLSTM1_G.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKyaNWaZMf3f"
      },
      "source": [
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hco5dFt14Q6Z"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(h.history['accuracy'])\n",
        "plt.plot(h.history['val_accuracy'])\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(h.history['loss'])\n",
        "plt.plot(h.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWMLiGmS-plO"
      },
      "source": [
        "from keras.models import load_model\n",
        "m=load_model('/content/drive/MyDrive/Colab Notebooks/BE_PROJECT/SM_CNNLSTM_G.h5')\n",
        "m.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFtRMC4wDsFx"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB_M2yo_Br4d"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag,word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('WordLemmatize')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "lemmatizer=WordNetLemmatizer()\n",
        "def preprocess_text(text):\n",
        "    stop_words = stopwords.words('english')\n",
        "    text_remove_symbol='\\$[A-Za-z]*'\n",
        "    text_remove_digits='[0-9]'\n",
        "    text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "    text_hash=r'#([^\\s]+)'\n",
        "    text=re.sub(text_remove_symbol,'',str(text).lower()).strip()\n",
        "    text=re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
        "    text=re.sub(text_hash,'',str(text)).strip()\n",
        "    text=re.sub(text_remove_digits,'',str(text)).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if  token not in stop_words and len(token)>3:\n",
        "               tokens.append(lemmatizer.lemmatize(token,get_wordnet_pos(token)))\n",
        "                \n",
        "    if len(tokens)<=3:\n",
        "        return np.nan\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "#RETURNS WHAT PART OF SPEECH IS THE WORD\n",
        "def get_wordnet_pos(word):\n",
        "    \n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict={\n",
        "    'J':wordnet.ADJ,\n",
        "    'N':wordnet.NOUN,\n",
        "    'V':wordnet.VERB,\n",
        "    'R':wordnet.ADV\n",
        "    }\n",
        "    return tag_dict.get(tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlL6FC8T9rmz"
      },
      "source": [
        "seq=token.texts_to_sequences([s])\n",
        "seq=pad_sequences(seq,maxlen=30)\n",
        "pred=model.predict(seq)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S96dNS59rkU"
      },
      "source": [
        "s='$FB Just like that money transfers from $AAPL  and $AMZN to FB..  this will fly high Monday social media isn&#39;t stopping anytime soon ðŸ¤‘'\n",
        "s=preprocess_text(s)\n",
        "print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyO0Sic89riH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtFnltQE9rfq"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usGd11BGu_H1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d89dc70-f5fa-4c67-d5ee-6887bb4b911a"
      },
      "source": [
        "api = twitter.Api(consumer_key='gLfZFnqWAcFqJQ2VtlbKtG1n7',\n",
        "                 consumer_secret='ND0Ld0ZzjA8I9gn0fBNNsgAQNrdYTgM7sfqPosYoODaEoPIPtL',\n",
        "                 access_token_key='2331621302-fs8VOMckbASRCimYQyxbDXdoLPckea4k5SYusMD',\n",
        "                 access_token_secret='Ww09Z4FnZplxAzdp53nKOJB2xDg32WQjD2VSFTYc8dHIb')\n",
        "\n",
        "# print(consumer_key,consumer_secret,access_token_key,access_token_secret)\n",
        "\n",
        "# test authentication\n",
        "print(api.VerifyCredentials())\n",
        "# print(twitter_api.GetStatus(126402758403305000))\n",
        "\n",
        "def create_Test(search):\n",
        "  try:\n",
        "    fetched=api.GetSearch(search,count=98,lang=\"en\",result_type=\"popular\")\n",
        "    print(len(fetched))\n",
        "    return [{\"id\":status.id,\"text\":status.text,\"label\":None} for status in fetched]\n",
        "  except:\n",
        "    return None\n",
        "a=create_Test('$APPL')\n",
        "print(a)\n",
        "def preprocess_text(text):\n",
        "    stop_words = stopwords.words('english')\n",
        "    text_remove_symbol='\\$[A-Za-z]*'\n",
        "    text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "    text_hash=r'#([^\\s]+)'\n",
        "    text=re.sub(text_remove_symbol,'',str(text).lower()).strip()\n",
        "    text=re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
        "    text=re.sub(text_hash,' ',str(text)).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if  token not in stop_words:\n",
        "            tokens.append(lemmatizer.lemmatize(token,get_wordnet_pos(token)))\n",
        "    return \" \".join(tokens)\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict={\n",
        "        'J':wordnet.ADJ,\n",
        "        'N':wordnet.NOUN,\n",
        "        'V':wordnet.VERB,\n",
        "        'R':wordnet.ADV\n",
        "        }\n",
        "    return tag_dict.get(tag,wordnet.NOUN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"created_at\": \"Fri Feb 07 09:58:39 +0000 2014\", \"default_profile\": true, \"favourites_count\": 1, \"followers_count\": 11, \"friends_count\": 37, \"id\": 2331621302, \"id_str\": \"2331621302\", \"name\": \"Abdul Dalvi\", \"profile_background_color\": \"C0DEED\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/431732476038221824/gUvU1U2Z_normal.jpeg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/431732476038221824/gUvU1U2Z_normal.jpeg\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"screen_name\": \"AbdulDalvi\", \"status\": {\"created_at\": \"Sat Sep 17 03:08:20 +0000 2016\", \"favorite_count\": 1, \"id\": 776981084818935808, \"id_str\": \"776981084818935808\", \"lang\": \"en\", \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\", \"text\": \"Join the movement. Take action and become a #globalcitizen. https://t.co/dk7k5gvBFG\"}, \"statuses_count\": 3, \"withheld_in_countries\": []}\n",
            "0\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja3WXttCd3j1",
        "outputId": "8f9ab3c1-b6b4-4650-f3b7-1488d5078647"
      },
      "source": [
        "a=create_Test('$AMZN')\n",
        "d=pd.DataFrame(a)\n",
        "d.head()\n",
        "d.text=d.text.apply(lambda x: preprocess_text(x))\n",
        "d.head()\n",
        "d.drop(columns=['id'])\n",
        "x_test=d['text']\n",
        "seq=token.texts_to_sequences(x_test)\n",
        "x_test=pad_sequences(seq,maxlen=30)\n",
        "y_pred=model.predict(x_test)\n",
        "d['label']=y_pred\n",
        "print(d.head())\n",
        "print(d.tail())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n",
            "                    id  ...     label\n",
            "0  1387866688289484802  ...  0.329763\n",
            "1  1387865015118614529  ...  0.553776\n",
            "2  1387859839582408712  ...  0.572320\n",
            "3  1387862284270194689  ...  0.369898\n",
            "4  1387862993644527620  ...  0.677585\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "                     id  ...     label\n",
            "9   1387861175334023169  ...  0.735956\n",
            "10  1387861947903848455  ...  0.346384\n",
            "11  1387903226985566209  ...  0.173875\n",
            "12  1387809827662057478  ...  0.255178\n",
            "13  1387896028704251911  ...  0.552151\n",
            "\n",
            "[5 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwP0t3Bfga8d"
      },
      "source": [
        "d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4WdZGAddECE",
        "outputId": "3a901eb6-c671-4bd1-a191-470235629924"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('WordLemmatize')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag,word_tokenize \n",
        "lemmatizer=WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading WordLemmatize: Package 'WordLemmatize' not\n",
            "[nltk_data]     found in index\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "5Lp2-US7adn3",
        "outputId": "10aefa66-4554-4ed9-b542-b3daee40e548"
      },
      "source": [
        "d=pd.DataFrame(a)\n",
        "d.head()\n",
        "d.text=d.text.apply(lambda x: preprocess_text(x))\n",
        "d.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1387751848493424641</td>\n",
              "      <td>incoming qualcomm ceo cristianoamon join u fir...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1387751853752946695</td>\n",
              "      <td>morning invest live biden 7 200 stimulus ubi e...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1387831335943876610</td>\n",
              "      <td>fantastic quarter every single dimension say b...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    id                                               text label\n",
              "0  1387751848493424641  incoming qualcomm ceo cristianoamon join u fir...  None\n",
              "1  1387751853752946695  morning invest live biden 7 200 stimulus ubi e...  None\n",
              "2  1387831335943876610  fantastic quarter every single dimension say b...  None"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hew706mWdbtA",
        "outputId": "999cc0fa-902a-4da0-847b-9cd535313905"
      },
      "source": [
        "print(x_test[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fantastic quarter every single dimension say bernstein senior research analyst toni sacconaghi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2_3vsvmYvF6"
      },
      "source": [
        "m=model.save('/content/drive/MyDrive/Colab Notebooks/BE_PROJECT/sent_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uMMKX6-u_FY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67fd358d-ba5a-4db7-c61f-a484e6806c10"
      },
      "source": [
        "s=model.predict(x_test,verbose=1,batch_size=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "320000/320000 [==============================] - 51s 160us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J65VlctgxG9A"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ_dM0k-xG5h"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o7IxCAOxG2m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "31b5da20-1411-46fb-a7f7-fa6e1ec477d3"
      },
      "source": [
        "x_test[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,   64,  203,  426, 1759,  143,   38,   36], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrZ6qPW2z5Zc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab029d84-7135-4094-c519-1d0aa7f6a195"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier(max_depth=5, n_estimators = 100)\n",
        "clf.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=5, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbLuGuLC-LR6",
        "outputId": "49d20003-5a5f-4fdc-ec54-529f78408537"
      },
      "source": [
        "from sklearn import metrics\n",
        "y_pred=clf.predict(x_test)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7268875917906232\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}